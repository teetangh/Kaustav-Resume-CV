# RAG Evaluation Pipeline

**Type:** Professional project (Couchbase)
**Timeline:** ~2024 -- 2025

---

## Summary

Comprehensive evaluation pipeline for Retrieval-Augmented Generation (RAG) applications, using Arize Phoenix for automated quality scoring.

---

## Evaluation Dimensions

- **Relevance** — how relevant are the retrieved documents to the query
- **Hallucination detection** — does the LLM generate claims not supported by the context
- **QA accuracy** — correctness of generated answers
- **Toxicity** — screening for harmful or inappropriate content

---

## Key Details

- Establishes quality gates for LLM-powered applications
- Automated scoring pipeline (not manual review)
- Integrates with Couchbase vector search for retrieval evaluation
- Uses **Arize Phoenix** as the evaluation framework

---

## Technologies

- Arize Phoenix
- Couchbase Capella (vector search)
- Python, LLM APIs

---

## Resume Appearance

| Resume Version | Included |
|---------------|----------|
| 2025 ai | Yes (Couchbase bullet 2) |
| 2025 combined | Yes (Couchbase bullet 1, merged) |
| 2026-02 ai | Yes (Couchbase bullet 2) |
| 2026-02 combined | Yes (Couchbase bullet 1, merged) |
| Dev variants | Not included |
